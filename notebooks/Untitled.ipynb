{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\voiceauth\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "F:\\Anaconda\\envs\\voiceauth\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Activation, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import constants\n",
    "from AudioManager import AudioManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_audio(language, sex, person_number, track, target=False):\n",
    "    print(f\"Loadig sample of {language} {sex}, person: {person_number}, track: {track}\")\n",
    "    audio_manager = AudioManager()\n",
    "    series = audio_manager.load_sample(\n",
    "        language=language,\n",
    "        sex=sex,\n",
    "        person_number=person_number,\n",
    "        track=track\n",
    "    )\n",
    "    del audio_manager\n",
    "\n",
    "    data = librosa.stft(series, n_fft=constants.MODEL_NFFT).swapaxes(0, 1)\n",
    "    samples = []\n",
    "\n",
    "    for i in range(0, len(data) - constants.BLOCK_LENGTH, constants.BLOCK_OVERLAP):\n",
    "        samples.append(numpy.abs(data[i:i + constants.BLOCK_LENGTH]))\n",
    "\n",
    "    results_shape = (len(samples), 1)\n",
    "    results = numpy.ones(results_shape) if target else numpy.zeros(results_shape)\n",
    "    return numpy.array(samples), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "voices = [\n",
    "    (\"japanese\", \"female\", 1, 1, True),\n",
    "    (\"japanese\", \"female\", 1, 2, True),\n",
    "    (\"japanese\", \"female\", 1, 3, True),\n",
    "    (\"french\", \"male\", 1, 1, False),\n",
    "    (\"french\", \"male\", 1, 2, False),\n",
    "    (\"english\", \"male\", 1, 1, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadig sample of japanese female, person: 1, track: 1\n"
     ]
    }
   ],
   "source": [
    "X, Y = prepare_audio(*voices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadig sample of japanese female, person: 1, track: 2\n",
      "Loadig sample of japanese female, person: 1, track: 3\n",
      "Loadig sample of french male, person: 1, track: 1\n",
      "Loadig sample of french male, person: 1, track: 2\n",
      "Loadig sample of english male, person: 1, track: 1\n"
     ]
    }
   ],
   "source": [
    "for voice in voices[1:]:\n",
    "    dx, dy = prepare_audio(*voice)\n",
    "    X = numpy.concatenate((X, dx), axis=0)\n",
    "    Y = numpy.concatenate((Y, dy), axis=0)\n",
    "    del dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = numpy.random.permutation(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=X.shape[1:]))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('hard_sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x25d6960ab88>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.005), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39 samples, validate on 10 samples\n",
      "Epoch 1/15\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.7897 - accuracy: 0.4615 - val_loss: 0.5345 - val_accuracy: 0.6000\n",
      "Epoch 2/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.5738 - accuracy: 0.6923 - val_loss: 0.4387 - val_accuracy: 0.8000\n",
      "Epoch 3/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.7949 - val_loss: 0.5270 - val_accuracy: 0.7000\n",
      "Epoch 4/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9744 - val_loss: 0.2561 - val_accuracy: 0.9000\n",
      "Epoch 5/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1640 - accuracy: 0.9744 - val_loss: 0.0735 - val_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9744 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1307 - accuracy: 0.9744 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1217 - accuracy: 0.9744 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.9487 - val_loss: 1.5518 - val_accuracy: 0.9000\n",
      "Epoch 10/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.9487 - val_loss: 1.5472 - val_accuracy: 0.9000\n",
      "Epoch 11/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.9487 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.4962 - accuracy: 0.9487 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.9744 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1045 - accuracy: 0.9744 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "39/39 [==============================] - 0s 2ms/step - loss: 0.1074 - accuracy: 0.9744 - val_loss: 0.0075 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25d69534ec8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=15, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 531us/step\n",
      "[0.09095557904517164, 0.9795918464660645]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadig sample of japanese female, person: 1, track: 3\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X, Y = prepare_audio(*voices[2])\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 16, 128)           328704    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 383,329\n",
      "Trainable params: 383,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
