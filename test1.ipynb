{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification: Text Independent Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human speech signal conveys many levels of information.  At the base level it carries a message in words.  But at other levels, it conveys information about language, dialect, emotion, gender and identity of the speaker.  While the speech recognition systems aim to identify the words spoken in the speech, the goal of the speaker recognition system is to extract the identity of the speaker associated with the speech signal.\n",
    "\n",
    "The broad area of speaker recognition emcompasses two more fundamental tasks. Speaker verification (also known as speaker authentication) is a task of determining whether a person is who she claims to be.  Speaker identification is a task of determining who is speaking from a known set of speakers.  The unknown speaker makes no identity claim so the system must perform a 1:N classification.\n",
    "\n",
    "These tasks can be further divided into text dependent and text independent categories. In a text dependent system the recognition system has prior knowledge of the text being spoken to.  In a text independent the recognition system is agnostic to the associated text.\n",
    "\n",
    "Our focus is the problem of speaker identification in the text independent context.  Further, we will concentrate this study on short speeches (usually 2-5 seconds) from a large number of speakers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our choice of audio dataset is open source VoxForge dataset.  It is freely available under GNU General Public License.  VoxForge was set up to collect transcribed speech for use in Open Source Speech Recognition Engines (\"SRE\"s).  The dataset contains 1216 unique speakerâ€™s multiple audio files in wav format.  Each speech is of short duration (2-10 seconds).  \n",
    "\n",
    "The voxforge dataset contains few samples where the speakers are not known and hence grouped under anonymous category.  We decided to exclude these samples from our project since they just impede the learning.  During the pre-processing stage, the wav files are converted to Mel-frequency cepstral coefficients (MFCCs) matrix of shape 20x196x1.  MFCCs can approximate the human auditory system response more closely than the linearly-spaced frequency bands used in the normal cepstrum. We experimented with Filter Bank energies as alternate but our findings indicate that for the speaker recognition task, the MFCC provides better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import tarfile\n",
    "import librosa.display\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxforge_root points to the root folder of the voxforge dataset.\n",
    "voxforge_root = Path('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(file_path):\n",
    "    \"\"\"\n",
    "    returns True if a regular files. False for hidden files.\n",
    "    Also, True is a known user with a name, False if anon.\n",
    "    \"\"\"\n",
    "    file_name = tf.strings.split(file_path, '\\\\')[1]\n",
    "    if tf.strings.substr(file_name, 0, 1) == tf.constant(b'.'):\n",
    "        return False\n",
    "    sc = tf.strings.split(file_path, '\\\\')[-3]\n",
    "    speaker = tf.strings.split(sc, '-')[0]\n",
    "    return not tf.strings.substr(speaker, 0, 9) == tf.constant(b'anonymous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Input to reshape is a tensor with 5 values, but the requested shape has 1\n\t [[{{node Reshape}}]]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001B[0m in \u001B[0;36mexecution_mode\u001B[1;34m(mode)\u001B[0m\n\u001B[0;32m   2112\u001B[0m       \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecutor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexecutor_new\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2113\u001B[1;33m       \u001B[1;32myield\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2114\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m_next_internal\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    729\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecution_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSYNC\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 730\u001B[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001B[0m\u001B[0;32m    731\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001B[0m in \u001B[0;36miterator_get_next\u001B[1;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[0;32m   2577\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2578\u001B[1;33m       \u001B[0m_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2579\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   6861\u001B[0m   \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6862\u001B[1;33m   \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   6863\u001B[0m   \u001B[1;31m# pylint: enable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\six.py\u001B[0m in \u001B[0;36mraise_from\u001B[1;34m(value, from_value)\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Input to reshape is a tensor with 5 values, but the requested shape has 1\n\t [[{{node Reshape}}]] [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-53-1cf363b3cce9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mlist_ds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_tensors\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mlist_ds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist_ds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mis_valid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlist_ds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m   \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    745\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    746\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 747\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    748\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOutOfRangeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    749\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m_next_internal\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    737\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_element_spec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_from_compatible_tensor_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mret\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    738\u001B[0m       \u001B[1;32mexcept\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 739\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mstructure\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_compatible_tensor_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_element_spec\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mret\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    740\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    741\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.pyenv\\pyenv-win\\versions\\3.8.0\\lib\\contextlib.py\u001B[0m in \u001B[0;36m__exit__\u001B[1;34m(self, type, value, traceback)\u001B[0m\n\u001B[0;32m    129\u001B[0m                 \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 131\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mthrow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraceback\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    132\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mexc\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    133\u001B[0m                 \u001B[1;31m# Suppress StopIteration *unless* it's the same exception that\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001B[0m in \u001B[0;36mexecution_mode\u001B[1;34m(mode)\u001B[0m\n\u001B[0;32m   2114\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2115\u001B[0m       \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecutor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mexecutor_old\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2116\u001B[1;33m       \u001B[0mexecutor_new\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2117\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2118\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\repos\\voiceauth\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py\u001B[0m in \u001B[0;36mwait\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     67\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m     \u001B[1;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m     \u001B[0mpywrap_tfe\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTFE_ExecutorWaitForAllPendingNodes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mclear_error\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Input to reshape is a tensor with 5 values, but the requested shape has 1\n\t [[{{node Reshape}}]]"
     ]
    }
   ],
   "source": [
    "files = glob('**/*.wav', recursive=True)\n",
    "tensors = [tf.constant(file) for file in files]\n",
    "list_ds = tf.data.Dataset.from_tensors(tensors)\n",
    "list_ds = list_ds.filter(is_valid)\n",
    "for f in list_ds.take(3):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaker(file_path):\n",
    "    ''' extract speaker name from the file path '''\n",
    "    sc = tf.strings.split(file_path, '/')[-3]\n",
    "    return tf.strings.split(sc, '-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each folder under root contains audio files for a speaker.\n",
    "# the folder name is the name of the speaker plus date and three digit code separated by dash.\n",
    "# let's print few sample speaker names.\n",
    "speaker_ds = list_ds.map(extract_speaker)\n",
    "for speaker in speaker_ds.take(3):\n",
    "    print(speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one-hot vector dataset from speakers\n",
    "speaker_encoder = preprocessing.LabelEncoder()\n",
    "speaker_idx = speaker_encoder.fit_transform([bytes.decode(s.numpy()) for s in speaker_ds])\n",
    "encoded_speaker_ds = tf.data.Dataset.from_tensor_slices(speaker_idx)\n",
    "unique_speakers = len(speaker_encoder.classes_)\n",
    "for es in encoded_speaker_ds.take(3):\n",
    "    print(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's listen to a clip from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio = os.path.join(voxforge_root, 'chocoholic-20070523/wav/rom0001.wav')\n",
    "import IPython.display as ipd\n",
    "ipd.Audio(sample_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the audio array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, sr = librosa.load(sample_audio)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a spectrogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = librosa.stft(x)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2mfcc(file_path, max_pad_len=196):\n",
    "    \"\"\" convert wav file to mfcc matrix with truncation and padding \"\"\"\n",
    "    wave, sample_rate = librosa.load(file_path, mono=True, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(wave, sample_rate)\n",
    "    mfcc = mfcc[:, :max_pad_len]\n",
    "    pad_width = max_pad_len - mfcc.shape[1]\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path):\n",
    "    \"\"\" returns 3D tensor of the mfcc coding from the wav file \"\"\"\n",
    "    file_name = bytes.decode(file_path.numpy())\n",
    "    mfcc = tf.convert_to_tensor(wav2mfcc(file_name))\n",
    "    mfcc = tf.expand_dims(mfcc, 2)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_ds(list_ds):\n",
    "    \"\"\" creates audio dataset containing audio tensors from file list dataset \"\"\"\n",
    "    batch = []\n",
    "    for f in list_ds:\n",
    "        audio = extract_mfcc(f)\n",
    "        batch.append(audio)\n",
    "    return tf.data.Dataset.from_tensor_slices(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time audio_ds = create_audio_ds(list_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio (input) tensor is 3D tensor.\n",
    "# 20x196 is MFCC encoding. Converting it to 3D for use in CNN layers.\n",
    "for a in audio_ds.take(1):\n",
    "    print(a.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, zip the input and labels to a single dataset.\n",
    "complete_labeled_ds = tf.data.Dataset.zip((audio_ds, encoded_speaker_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = None\n",
    "for audio, speaker in complete_labeled_ds.take(1):\n",
    "    input_shape = audio.shape\n",
    "    print('input_shape', audio.shape)\n",
    "    print('output_shape', speaker.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing we just few samples.\n",
    "#labeled_ds = complete_labeled_ds.take(3000)\n",
    "# for complete run with all samples.\n",
    "labeled_ds = complete_labeled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validation and test datasets.\n",
    "data_size = sum([1 for _ in labeled_ds])\n",
    "train_size = int(data_size * 0.9)\n",
    "val_size = int(data_size * 0.05)\n",
    "test_size = data_size - train_size - val_size\n",
    "print('all samples: {}'.format(data_size))\n",
    "print('training samples: {}'.format(train_size))\n",
    "print('validation samples: {}'.format(val_size))\n",
    "print('test samples: {}'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batched datasets\n",
    "batch_size = 32\n",
    "labeled_ds = labeled_ds.shuffle(data_size, seed=42)\n",
    "train_ds = labeled_ds.take(train_size).shuffle(1000).batch(batch_size).prefetch(1)\n",
    "val_ds = labeled_ds.skip(train_size).take(val_size).batch(batch_size).prefetch(1)\n",
    "test_ds = labeled_ds.skip(train_size + val_size).take(test_size).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    dropout_rate = .25\n",
    "    regularazation = 0.001\n",
    "    audio_input = keras.layers.Input(shape=input_shape)\n",
    "    conv1 = keras.layers.Conv2D(16, kernel_size=(3, 3), padding='same',\n",
    "                               activation='relu', input_shape=input_shape)(audio_input)\n",
    "    maxpool1 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(conv1)\n",
    "    batch1 = keras.layers.BatchNormalization()(maxpool1)\n",
    "    conv2 = keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same',\n",
    "                               activation='relu', input_shape=input_shape)(batch1)\n",
    "    maxpool2 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(conv2)\n",
    "    batch2 = keras.layers.BatchNormalization()(maxpool2)\n",
    "    conv3 = keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', \n",
    "                activation='relu')(batch2)\n",
    "    maxpool3 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(conv3)\n",
    "    batch3 = keras.layers.BatchNormalization()(maxpool3)\n",
    "    flt = keras.layers.Flatten()(batch3)\n",
    "    drp1 = keras.layers.Dropout(dropout_rate)(flt)\n",
    "    dense1 = keras.layers.Dense(unique_speakers * 2, activation='relu',\n",
    "                kernel_regularizer=keras.regularizers.l2(regularazation))(drp1)\n",
    "    drp2 = keras.layers.Dropout(dropout_rate)(dense1)\n",
    "    output = keras.layers.Dense(unique_speakers, activation='softmax', name='speaker')(drp2)\n",
    "    model = keras.Model(inputs=audio_input, outputs=output)\n",
    "    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if previously trained model is on the disk, use it without training.\n",
    "# the model has millions of parameters and training with 32 epoches\n",
    "# takes 12hrs+ on my mac. \n",
    "train_model = False\n",
    "model_name = 'spr_model.h5'\n",
    "model_path = os.path.join('.', model_name)\n",
    "model = None\n",
    "if os.path.exists(model_path):\n",
    "    model = keras.models.load_model(model_path)\n",
    "else:\n",
    "    model = create_model()\n",
    "    train_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if training, you can view the tensorboard.\n",
    "# type on command line to start tensorboard: tensorboard --logdir=./spr_logs --port=6006\n",
    "# view details: http://localhost:6006\n",
    "if train_model:\n",
    "    root_logdir = os.path.join(os.curdir, \"spr_logs\")\n",
    "    def get_run_dir():\n",
    "        import time\n",
    "        run_id = time.strftime(\"run%Y_%m_%d-%H_%M_%S\")\n",
    "        return os.path.join(root_logdir, run_id)\n",
    "    run_logdir = get_run_dir()\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(run_logdir, update_freq='batch')\n",
    "    history = model.fit(train_ds, epochs=8, validation_data=val_ds, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model:\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test few files ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = [os.path.join(voxforge_root,'mk-20120531-ctv/wav/a0369.wav'),\n",
    "               os.path.join(voxforge_root,'rocketman768-20080408-axr/wav/b0220.wav')]\n",
    "sample_ds = tf.data.Dataset.from_tensor_slices(sample_file)\n",
    "sample_input = create_audio_ds(sample_ds).batch(2)\n",
    "output = model.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = output.argmax(axis=1)\n",
    "speakers = speaker_encoder.inverse_transform(speaker_ids)\n",
    "print(speakers)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for your time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}